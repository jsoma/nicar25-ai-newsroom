{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More models and tools\n",
    "\n",
    "AI is more than just large language model chatbots!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper\n",
    "\n",
    "OpenAI has released other AI tools besides ChatGPT – one of the most popular is [Whisper](https://openai.com/research/whisper), a model that can **transcribe audio**. The fact, technical name for this is \"speech to text.\"\n",
    "\n",
    "Unlike GPT, **you can actually download and use Whisper**. Python programmers can bop on over to [the GitHub repo](https://github.com/openai/whisper) and coding with it minutes.\n",
    "\n",
    "Because Whisper is an open model (definition *to be discussed*), you'll see all sorts of Whisper-powered tools out there. [MacWhisper](https://goodsnooze.gumroad.com/l/macwhisper) allows you to transcribe audio from the safety of your mac - powered by Whisper! [This random website](https://whisperui.com/) allows to drag-and-drop audio files and transcribe them on the web – powered by Whisper!\n",
    "\n",
    "And now we'll do the exact same thing right here, in Python – powered by Whisper! We'll start by **installing it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet openai-whisper torch torchaudio whisperx pandas yt-dlp pydantic transformers torch sentence-transformers sacremoses timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like spaCy or the Hugging Face models, Whisper isn't just one piece of software - it's a collection of models with different sizes and names that you have to download separately. When we use `whisper.load_model` below it will run out on the internet and grab the model we're asking for.\n",
    "\n",
    "You can see [the models here](https://github.com/openai/whisper?tab=readme-ov-file#available-models-and-languages). We're going to start with `tiny.en`, an English-only model that is the smallest and fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"tiny.en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the audio we're going to transcribe. Yes, it's *very* short and not terribly complicated.\n",
    "\n",
    "<audio controls src=\"sample-4.mp3\"></audio>\n",
    "\n",
    "The actual transcribing is just one line! We'll use `%%time` at the top of the cell to see how long it takes, so later we can compare the `tiny.en` model with some other, larger ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = model.transcribe(\"sample-4.mp3\")\n",
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 seconds of audio transcribed in about 1 second! Pretty good, *except* for the fact that it says the incorrect \"We've thrown\" instead of the correct \"We frown.\"\n",
    "\n",
    "Let's try again with a slightly larger model, the medium English-only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = model.transcribe(\"sample-4.mp3\")\n",
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing to a slightly larger model really impacted our time! It took 4 seconds for a 2-second audio clip. But on the upside it was at least *correct*.\n",
    "\n",
    "You can try [other models](https://github.com/openai/whisper?tab=readme-ov-file#available-models-and-languages), too. The non-`.en` ones are multilingual (to varying degrees), give them a shot as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = model.transcribe(\"sample-4.mp3\")\n",
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see various metrics about how good the transcription abilities are for each language, including CER, WER, BLEU and other scores. One thing to note is that in transcription a 80% score is far worse than a 80% score on, say, a math test. Having one out of every ten words be wrong is... not great in practice.\n",
    "\n",
    "Never listen to scores when dealing with transcription tools, **always test them in the field.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But... Whisper is actually bad!\n",
    "\n",
    "[According to everyone](https://apnews.com/article/ai-artificial-intelligence-health-business-90020cdf5fa16c79ca2e5b6c4c9bbb14), and the excellently-named paper [Careless Whisper: Speech-to-Text Hallucination Harms](https://dl.acm.org/doi/10.1145/3630106.3658996), Whiper makes *a lot of bad mistakes.*\n",
    "\n",
    "> In an example they uncovered, a speaker said, “He, the boy, was going to, I’m not sure exactly, take the umbrella.”\n",
    ">\n",
    "> But the transcription software added: “He took a big piece of a cross, a teeny, small piece ... I’m sure he didn’t have a terror knife so he killed a number of people.”\n",
    "\n",
    "One of the biggest problems is **silence**. Like human beings, Whisper isn't very good at silence! It's trained to transcribe transcribe transcribe, so when there's silence it tends to start writing regardless of what's going on.\n",
    "\n",
    "One way to fix this is **voice activity detection**, which cuts out silences before it transcribes.\n",
    "\n",
    "Remember how I said Whisper was open source, and other people could build tools on top of it? As a result, we have great libraries like [WhisperX](https://github.com/m-bain/whisperX) which had add-ons like VAD, speaker diarization (splitting speakers!) and more. It's a little more unwieldy to use, but it's worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 16 if device == \"cuda\" else 4\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\" \n",
    "\n",
    "# 1. Transcribe with original whisper\n",
    "model = whisperx.load_model(\"turbo\", device, compute_type=compute_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"How to Fix Holes in Drywall - 4 Easy Methods [uvQK7WTkKpI].webm\"\n",
    "\n",
    "audio = whisperx.load_audio(audio_file)\n",
    "result = model.transcribe(audio, batch_size=batch_size)\n",
    "print(\"Transcribed\")\n",
    "\n",
    "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "print(\"Aligned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.json_normalize(result['segments'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just grab the text\n",
    "text = \"\\n\".join(df['text'])\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to a file\n",
    "with open(\"transcript.txt\", \"w\") as fp:\n",
    "    fp.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait, where did that video come from?\n",
    "\n",
    "[yt-dlp](https://github.com/yt-dlp/yt-dlp) is the best tool for downloading video (and audio) from YouTube, TikTok, or anywhere else. I always use ChatGPT to write the code for me because I can't remember all the little bits and pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=uvQK7WTkKpI\"\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestvideo[height<=720]+bestaudio/best[height<=720]',  # Max 720p\n",
    "    'outtmpl': '%(title)s.%(ext)s',  # Save file as video title\n",
    "    'merge_output_format': 'mp4',  # Ensure MP4 format\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want **just the audio:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=uvQK7WTkKpI\"\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': '%(title)s.%(ext)s',\n",
    "    'postprocessors': [\n",
    "        {'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3'},  # Converts to MP3\n",
    "        {'key': 'FFmpegMetadata'},  # Embeds metadata\n",
    "    ],\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting with last session's LLM knowledge\n",
    "\n",
    "Back to our transcript: **that text is so long!** We don't want to read it! We'd rather have a nice, simple summary.\n",
    "\n",
    "So: **let's ask for one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new dataset\n",
    "\n",
    "with open(\"transcript.txt\", \"r\") as fp:\n",
    "    transcript = fp.read()\n",
    "\n",
    "transcript[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new model\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List\n",
    "\n",
    "class TranscriptSummary(BaseModel):\n",
    "    \"\"\"Data model for a transcript.\"\"\"\n",
    "    topic: str = Field(description=\"One-sentence blurb about transcript content\")\n",
    "    summary: str = Field(description=\"Summary of content covered, covering all major points\")\n",
    "    highlights: str = Field(description=\"Most interesting fact(s) from transcript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new prompt\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'XXXXXXXX'\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = transcript\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the relevant information from the following transcript.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    response_format=TranscriptSummary,\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what if it's just too long?\n",
    "\n",
    "Each model has a **context window**, which is the amount of text it can handle at one time. To summarize you often use tools that break it up into sections, summarize each part, then give you a final, overall summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "\n",
    "summarizer = TreeSummarize(verbose=True, output_cls=TranscriptSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = summarizer.get_response(\"Summarize the following YouTube video transcript\", [transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about not using GPT?\n",
    "\n",
    "While GPT is the most *popular* tool, it isn't necessarily the best! Sometimes instead of media partnerships (OpenAI), you want more personality or coding abilities (Claude), or largest context windows and the ability to directly take in audio/images/video (Gemini). \n",
    "\n",
    "To use Gemini you''ll need a [Gemini API key](https://aistudio.google.com/app/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"XXXXXXXX\",\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What color is the sky?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new prompt\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"XXXXXXXX\",\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "prompt = transcript\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the relevant information from the following transcript.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    response_format=TranscriptSummary,\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But Gemini can also take audio directly!\n",
    "\n",
    "And video, for that matter! First we'll download the audio with yt-dlp (naming it directly after the ID this time), then we'll feed it into Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=uvQK7WTkKpI\"\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': '%(id)s.%(ext)s',\n",
    "    'postprocessors': [\n",
    "        {'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3'},  # Converts to MP3\n",
    "        {'key': 'FFmpegMetadata'},  # Embeds metadata\n",
    "    ],\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"XXXXXXXX\",\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "with open(\"uvQK7WTkKpI.mp3\", \"rb\") as audio_file:\n",
    "  base64_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            { \"type\": \"text\", \"text\": \"Extract the relevant information from the following YouTube audio\", },\n",
    "            { \"type\": \"input_audio\", \"input_audio\": { \"data\": base64_audio, \"format\": \"mp3\" } }\n",
    "        ],\n",
    "    }],\n",
    "    response_format=TranscriptSummary\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local models\n",
    "\n",
    "A \"local model\" is an LLM or similar tool that runs on your own computer, not out in the cloud.\n",
    "\n",
    "Local models are important to discuss about because **journalists are very cheap, and also obsessed with privacy**. If we can avoid using the cloud it might make sense to do so (although... maybe not?). To start, I'm going to link to [Deepseek right here](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF), `unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF` because the name is so absurd. Look at all those pieces:\n",
    "\n",
    "- **unsloth/:** the makers of unsloth produced some aspect of this\n",
    "- **Deepseek:** This is a Deepseek\n",
    "- **R1:** reasoning model which has been\n",
    "- **Distill:** distilled into\n",
    "- **Qwen:** a Qwen model\n",
    "- **1.5B:** that has 1.5 billion parameters\n",
    "- **GGUF:** transformed into a GGUF file format\n",
    "\n",
    "AFAIK the last one is the step from unsloth. **We'll talk about what each one of these means as we go on.** Or maybe, hopefully, potentially!\n",
    "\n",
    "Even though you *can* write Python code directly to use the LLMs in their current state, using a tool on top of it is going to make things a lot easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM Studio and Quantized models\n",
    "\n",
    "[LM Studio](https://github.com/lmstudio-ai) is a piece of software you load up on your computer that allows you to browse, download, and use LLM models. You can jump through all sorts of weird Python situations, but LM Studio is just *so easy* – you click around, it tells you what to do, what models will work, etc, and life is perfect.\n",
    "\n",
    "The goal of using LM Studio is to be able to track down models that fit on your computer, **even versions of big models that are sized to fit on your computer.**\n",
    "\n",
    "When I say \"big models,\" \"big\" refers to the **number of parameters**, or pieces of information that the model can take in at once. For example, a 7B model has 7 billion parameters, while a 300B model has 300 billion parameters. Larger models are typically slower and require fancier computers, but are almost always better.\n",
    "\n",
    "> This explanation of quantized models is *almost certainly very inaccurate*, but it's a good conceptual framework.\n",
    "\n",
    "To make these giant models fit on your computer, they are **quantized**, or made into smaller versions. To a large degree a quantized model is just a smaller versions of big models that's aaaaaalmost as capable. Because an LLM is a big magic box of numerical calculations, you can make it smaller by just **rounding off the numbers inside of it**. The original numbers are big andl ong and specific, like this:\n",
    "\n",
    "- 3.4967845395720573\n",
    "- 6.1857232150673637\n",
    "- 1.3792183003746249\n",
    "\n",
    "Quantized models take those numbers and just chops off the end. They typically come in q8, q6, a smattering of q4's, and q2 versions. Here's a completely fake example of what the quantized numbers might look like:\n",
    "\n",
    "|q8|q6|q4|q2|\n",
    "|---|---|---|---|\n",
    "|3.49678453|3.496784|3.4967|3.49|\n",
    "|6.18572321|6.185723|6.1857|6.18|\n",
    "|1.37921830|1.379218|1.3792|1.37|\n",
    "\n",
    "Let's say you have a 36B model which requires 40GB of RAM (actual memory, not disk space) to load into use on your computer. If you only have 16GB of RAM, that isn't going to work! You *could* go with the 7B model, but that's almost 80% fewer parameters, which means the model is a *lot* dumber. Instead, you'll go for a q4 version of the 36B model – the numbers are rounded off which makes the model a lot smaller, but more parameters is better than a higher quantization.\n",
    "\n",
    "Usually q4 is still fine, anything lower starts to know too little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcript.txt\") as fp:\n",
    "    transcript = fp.read()\n",
    "\n",
    "print(transcript[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll need to **load up LM Studio** and load your model. You can see some pictures of how to do that [here](https://jonathansoma.com/words/olmocr-on-macos-with-lm-studio.html) (although you won't be looking for olmocr). Just look for anything popular that fits on your computer!\n",
    "\n",
    "We talk to LM Studio using the same OpenAI library as when we talk to GPT – every tool seems to have standardized on the conversation format they use. We just set the `base_url` and `api_key` according to what LM Studio tells us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:10000/v1\",\n",
    "    api_key=\"lm-studio\"\n",
    ")\n",
    "\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# LM Studio is :1234, Msty is :10000\n",
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:10000/v1\",\n",
    "    api_key=\"lm-studio\"\n",
    ")\n",
    "\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Summarize this transcript in four descriptive bullet points.\"},\n",
    "        {\"role\": \"user\", \"content\": transcript},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's slow, but looks great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do with this information?\n",
    "\n",
    "- [GitHub Actions](https://github.com/features/actions)\n",
    "- [Mailgun](https://www.twilio.com/en-us/sendgrid/email-api)\n",
    "\n",
    "There are *two GitHub Actions sessions?* But they were in the past, so instead [watch this video](https://www.youtube.com/watch?v=QNKxzkNpsko&list=PLewNEVDy7gq17ju86mZqPzr2mGwVLwNNM&index=1) and [learn to use secrets](https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions) and you'll be good to go. Maybe a little [Claude] or [ChatGPT](https://chatgpt.com/) magic.\n",
    "\n",
    "> I have a script that transcribes an audio file and summarizes/gets bullet points out of it. I want to make it download the most recent file from a youtube playlist and email me the transcript/summary/highlights. I want this to happen every Monday morning. You're going to help me do this!\n",
    ">\n",
    "> The workflow should use GitHub Actions and Mailgun. I want to drag-and-drop my GitHub files because I don't really understand git. Step me through each part of the process. My code currently is in a Jupyter notebook, so you might have to ask me to copy and paste it in here so you can see what I'm doing. Make sure I copy all of the relevant portions.\n",
    ">\n",
    "> Let's do this **step by step**. Set out a plan and walk me through it, making sure I understand what's happening in each step. Don't proceed to the next step until you're comfortable that I've done what is necessary. Before each step in the process, ask if I have code that does it already or if we need to do it from scratch. Don't assume I know much of anything, as I'm working from a template/tutorial (e.g. I maybe have heard of yt-dlp, but don't really understand what it is)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face\n",
    "\n",
    "Hugging Face is like **YouTube for AI.** All the models go there!\n",
    "\n",
    "There are a [zillion models on Hugging Face](https://huggingface.co/models), but the top ones appear to be ALL text generation models. Since we know that [ones that cost money are the best](https://lmarena.ai/?leaderboard) we ignore them for now.\n",
    "\n",
    "By the second or third page you see a few text classification or sentence similarity models, mostly due to the popularity of \"retrieval augmented generation,\" the idea that we can ask a question to an LLM, it finds relevant sentences, then answers a question with them. We don't want to do that, either!\n",
    "\n",
    "As we scroll and scroll and scroll, we eventually come across [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli), which is a text classification model from Facebook. Most normal classification models only know some specific categories to put things in - positive tweets vs negative tweets, for example – but `facebook/bart-large-mnli` can categorize... anything?\n",
    "\n",
    "### Putting things in categories\n",
    "\n",
    "Now that we've settled on [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli), let's use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model, we're going to click the \"use in Transformers\" link on the top left. That will give us the base code for loading the model with the `pipeline` tool. Then we'll scroll down on the page itself to see if there's an example. And there is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"I'm tired from so much ballet, but it's time to make lunch\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tasks\n",
    "\n",
    "While we're used to just asking questions back and forth with ChatGPT all day, most questions involving language or images are actually pre-defined tasks that have been studies for decades. For example, \"put this text into a category\" is called **classification**.\n",
    "\n",
    "You can see a ton of examples of different machine learning tasks on Hugging Face's [tasks page](https://huggingface.co/tasks).\n",
    "\n",
    "### Translation\n",
    "\n",
    "For example, [translation is one option](https://huggingface.co/tasks/translation). It comes with a [small example](https://huggingface.co/tasks/translation#inference) that seems easy enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
    "en_fr_translator(\"How old are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I showed this to a French person and they *laughed!* It's a word by word translation, not actually how French is spoken. Such is the state of machine learning, *c'est la vie.*\n",
    "\n",
    "> It's probably important to think about how even though you can *an answer* from a tool like this, it doesn't mean it's a *good answer*. It's easy to be distracted by AI seeming fancy and confident, when really it's just a computer pushing numbers around!\n",
    "\n",
    "There's [another example on that page](https://huggingface.co/tasks/translation#inference), but they screwed it up! It uses another model that, if prompted, gives the correct translation of \"How old are you?\". On the page they I guess wanted to mix things up and changed it to translate \"How are you?\". We'll go with what was intended below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "translator(\"How old are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When translating, you request the pipeline `translation_xx_to_yy`, where `xx` is the source language and `yy` is the target language. Not all models support all languages, so you might have to [poke around for what you want](https://huggingface.co/models?pipeline_tag=translation) (the languages tab isn't even always the best route: sometimes the model you want is only filed under \"multilingual\").\n",
    "\n",
    "There are two English-Chinese models that are ranked high, one is [Helsinki-NLP/opus-mt-en-zh](https://huggingface.co/Helsinki-NLP/opus-mt-en-zh) and one is [Helsinki-NLP/opus-mt-zh-en](https://huggingface.co/Helsinki-NLP/opus-mt-zh-en). If we don't read the documentation we won't notice that for going from English to Chinese we need the first one, `opus-mt-en-zh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_zh\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "translator(\"How old are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the translation in the opposite direction by switching both the model and the pipeline name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_zh_to_en\", model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "translator(\"你几岁了?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try [this multilingual model](https://huggingface.co/facebook/nllb-200-distilled-600M) suddenly everything gets very crazy very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\n",
    "    \"translation_ja_to_en\",\n",
    "    model=\"facebook/nllb-200-distilled-600M\")\n",
    "translator(\"私は鉛筆です\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it so bad??? Because despite not telling us how to use the model, `translation_xx_to_yy` apparently is *not* how you use this model, and we apparently need to use [some other weird language codes](https://github.com/facebookresearch/flores/blob/main/flores200/README.md) that we pass in as `src_lang` and `tgt_lang`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\n",
    "    \"translation\",\n",
    "    model=\"facebook/nllb-200-distilled-600M\",\n",
    "    src_lang='jpn_Jpan',\n",
    "    tgt_lang='eng_Latn')\n",
    "translator(\"私は鉛筆です\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I honestly don't know how we were supposed to learn how to do this. I figured it out by reading [the code](https://huggingface.co/spaces/Geonmo/nllb-translation-demo/blob/main/app.py) of one of [the demo spaces](https://huggingface.co/spaces/Geonmo/nllb-translation-demo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait, what's a \"demo space?\"\n",
    "\n",
    "When you hear about new models or approaches that are released, you immediately want to try them. Like Deepseek! \n",
    "\n",
    "In the next notebook we'll see **how to effectively experiment with these tools**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "How can you trust anything?? Even if we're impressed by output from AI at first blush, it might not have the consitent quality necessary to perform *real* tasks. Or maybe it does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
