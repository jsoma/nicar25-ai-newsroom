{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the API, let's *talk about* the [OpenAI model playground](https://platform.openai.com/playground?mode=complete).\n",
    "\n",
    "## Working with the API\n",
    "\n",
    "The [chat interface](https://chat.openai.com/) is just one method for interacting with GPT. Another option is to write Python code that skips the website! This is called an **API**. Technically it stands for *Application Programming Interface*, but no one actually remembers that: we can just think of it as a way for two computers to talk directly to each out.\n",
    "\n",
    "To use the OpenAI GPT API with Python, we're going to need to install the [openai Python package](https://github.com/openai/openai-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet openai pandas pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking questions\n",
    "\n",
    "You use the Python interface just like the chat one: you start from a series of messages and ask the API what's next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"XXXXXXXX\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the ratio of water to vinegar used when making pickles?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The note above includes the **system prompt**, which are guidelines for GPT that set the tone for the conversation. [You can see some that people use here](https://github.com/mustvlad/ChatGPT-System-Prompts) (or rather, one of them), but we're just going to use the traditional \"You are a helpful assistant.\"\n",
    "\n",
    "> There's a whole... cottage industry? magic wizard alchemy industry? of people trying to develop good system prompts. I personally don't think it's worth spending time on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "People who use ChatGPT for free get access to a weaker version of GPT, while those who pay get access to a more powerful one. The powerful one can deal with more text at once (a report! a book!) and just generally gives better answers.\n",
    "\n",
    "These different versions are called **models**, and they're the tech that lives behind the interface. You can find more about [the available OpenAI models here](https://platform.openai.com/docs/models). When you're using the API, it's important to know [how much each one costs](https://platform.openai.com/docs/pricing).\n",
    "\n",
    "> **There's a new one out!** It's INCREDIBLY EXPENSIVE and also not very good?? OpenAI is not doing great, maybe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-4'\n",
    " )\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different companies each produce different, competing models: Anthropic has made [Claude](https://claude.ai/), Google has [Gemini](https://gemini.google.com/), Deepseek has [Deepseek](https://www.deepseek.com/)...\n",
    "\n",
    "People have models [fight it out](https://lmarena.ai/?leaderboard) and they're all pretty high up there.\n",
    "\n",
    "For many of these models, you're sending information off into the cloud every time you make a request or ask a question. Deepseek is the only current model topping the list that you can download and run on your own machine (depending on how powerful it is)! Notice how the [leaderboards](https://lmarena.ai/?leaderboard) start with famous ones that I've listed from big companies - GPT, Gemini â€“ but then descend into things you've never heard of like Qwen, LLaMa, Phi-4 (...which are also sometimes from big companies).\n",
    "\n",
    "Check the \"License\" column: **they're the non-proprietary ones.**\n",
    "\n",
    "Most concepts can be transferred from model to model, so just know that even if we're working with OpenAI's GPT in the examples below, we could just as well be working with another one (and maybe we'll give one a try later on!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "When you're chatting with [Claude](https://claude.ai/), it has four modes: normal, concise, explanatory, and formal. They're described like so:\n",
    "\n",
    "> - Normal: Default responses from Claude.\n",
    "> - Concise: Shorter responses & more messages.\n",
    "> - Explanatory: Educational responses for learning\n",
    "> - Formal: Clear and structured responses\n",
    "\n",
    "You can also [create your own styles](https://support.anthropic.com/en/articles/10181068-configuring-and-using-styles)\n",
    "You can imagine the system prompt - \"You are a helpful assistant.\" - being changed for each of these behind the scenes.\n",
    "\n",
    "Another change that can be made is **temperature**, which is how \"crazy\" you let your model get. [This Financial Times piece](https://ig.ft.com/generative-ai/) does a good job explaining how it's just a simple matter of statistics: based on the text it's seen so far, what's the most likely next word?\n",
    "\n",
    "The `temperature` setting allows you to use less likely words instead of the most likely next one. Even though it isn't the same as asking for more creative output, I like to think of them as being similar. Increasing the temperature makes the text more unpredictable, and potentially more creative!\n",
    "\n",
    "By default the temperature with GPT is 0.7, which allows a moderate amount of creativity. If you downgrade the temperature to 0.0, conversations will almost always produce the same result! The maximum is 2.0, which can get some pretty wild results [in the playground](https://platform.openai.com/playground)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a short story in one paragraph.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0\n",
    " )\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try the same prompt again, with the same `0.0` temperature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a short story in one paragraph.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0\n",
    " )\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're tired of being read the same bedtime story every single night, we can increase the temperature to allow GPT to pick less likely words each time it steps forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a short story in one paragraph.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.2\n",
    " )\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the further down the text it gets, the further from the original temperature `0.0` version we get! This is because those probabilities add up as you go deeper and deeper into the text, guiding the conversation into completely different paths.\n",
    "\n",
    "And if we want to get something completely different right out of the gate? Let's maximize the temperature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a short story in one paragraph.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=2.0\n",
    " )\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic categorization\n",
    "\n",
    "One of my favorite use cases for using the API is to **put things in categories**, which has the technical term of *classification*. Later we'll look at how to do this more formally, but let's try a DIY example now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"XXXXXXXX\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Categorize the following legislative bill as ENVIRONMENT, HEALTHCARE, IMMIGRATION, TAXES/FINES, or OTHER. \n",
    "\n",
    "Bill text: \n",
    "\n",
    "The \"Celestial Reforestation Act\" proposes the ambitious endeavor of transporting arboreal \n",
    "specimens beyond Earth's confines. Acknowledging the pivotal role of trees in sustaining \n",
    "ecosystems and combating ecological degradation, this bill outlines a strategic roadmap \n",
    "for the selection, launch, and maintenance of arboreal lifeforms into outer space. Through \n",
    "collaboration with aerospace entities and research institutions, this legislation aims to \n",
    "establish extraterrestrial arboreal habitats, facilitating scientific exploration and the \n",
    "expansion of green spaces beyond planetary boundaries. By fostering innovation in space biology\n",
    "and exploration, this act seeks to pioneer sustainable solutions for global challenges while \n",
    "advancing humanity's reach into the cosmos.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    { \"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original response says \"This legislative bill would be categorized as ENVIRONMENT,\" which is *not* okay. I want it to just say the category name!\n",
    "\n",
    "You can head on over to [ChatGPT itself](https://chat.openai.com) to engineer a good prompt. Re-run your model until you feel happy with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk processing\n",
    "\n",
    "Oftentimes you end up with a looooong spreadsheet or database of things that need to be categorized. But whether we're technical or not, it's easy to tackle!\n",
    "\n",
    "### Python/pandas\n",
    "\n",
    "> If you're not a Python person: it's fine! Just think about this in terms of concepts. You just want to be familiar with the idea of **api key** and a **prompt**.\n",
    "\n",
    "Let's say we have a dataset that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'title': [\n",
    "        'Trees in space',\n",
    "        'Taxes on people who are in outer space',\n",
    "        'Medical expenses for aliens',\n",
    "        'Pinecones orbiting the planet'\n",
    "    ]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add a new column to this, called `llm_category`. To run the code above for every row in a pandas dataframe, we make two adjustments to the code above:\n",
    "\n",
    "We build a template for our prompt, which now has a placeholder of `{text}` where our bill details will go:\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"\n",
    "Categorize the following legislative bill as ENVIRONMENT, HEALTHCARE, IMMIGRATION, TAXES/FINES, \n",
    "or OTHER. Only respond with the category name.\n",
    "\n",
    "Bill title: {text}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "A function called `llm_request`, which receives a single row of data and uses it to complete the template.\n",
    "\n",
    "```python\n",
    "prompt = prompt_template.format(text=row['title'])\n",
    "```\n",
    "\n",
    "That prompt is then sent to the LLM and the result returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"XXXXXXXX\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Categorize the following legislative bill as ENVIRONMENT, HEALTHCARE, IMMIGRATION, TAXES/FINES, \n",
    "or OTHER. Only respond with the category name.\n",
    "\n",
    "Bill title: {text}\n",
    "\"\"\"\n",
    "\n",
    "def llm_request(row):\n",
    "    prompt = prompt_template.format(text=row['title'])\n",
    "    \n",
    "    messages = [\n",
    "        { \"role\": \"system\", \"content\": \"You are a legislative assistant.\"},\n",
    "        { \"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try it out with the first row of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = df.loc[0]\n",
    "\n",
    "print(first_row)\n",
    "llm_request(first_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with a made-up row, just to be able to experiment a little more freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_request({'title': 'The bill to let people from Mars move to planet Earth'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're happy with how it works in small doses, we can move on to using it with every row. We're going to use the Python library [tqdm](https://github.com/tqdm/tqdm) to get some nice progress bars while it chugs along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "if 'answer' not in df.columns:\n",
    "    df['answer'] = None\n",
    "\n",
    "# Get rows that need processing\n",
    "mask = df['answer'].isna()\n",
    "\n",
    "# Process with itertuples (faster than iterrows)\n",
    "for row in tqdm(df[mask].itertuples(), total=mask.sum()):\n",
    "    try:\n",
    "        result = llm_request(row)\n",
    "        \n",
    "        # Update the original dataframe using the Index attribute\n",
    "        df.loc[row.index, 'answer'] = result\n",
    "    except Exception as e:\n",
    "        print(f\"Error on row {row.Index}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's magic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google sheets\n",
    "\n",
    "I personally use [GPT for work](https://gptforwork.com/). I'll show you a demo, but I don't think we can install things on your Google account without getting a little too crazy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the results\n",
    "\n",
    "We can't just say, \"oh wow, computers seem cool, let's just trust whatever it says!\" Our job as journalists who request trust from our audience is to **actually test the results.**\n",
    "\n",
    "We can do this one of two ways:\n",
    "\n",
    "1. Run the classifier over all of our bills, then take a sample of the results to hand-label and compare with the LLM's judgment\n",
    "2. Have a small hand-labeled test dataset that we use to verify the LLM's results before moving on to classify everything.\n",
    "\n",
    "The second path is usually the best since it allows you to tweak your results before running your prompt against *everything*. The first path is useful if you have a workflow already and need to check whether it's [still working](https://www.reddit.com/r/ChatGPT/comments/182ubh7/chatgpt_has_become_unusably_lazy/) or has [shifted unexpectedly](https://arxiv.org/abs/2307.09009)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/labeled-bills.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our dataset of human-labeled content and see what the AI thinks about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['llm_category'] = df.progress_apply(llm_request, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the two compare? We'll use `crosstab` to do it here, but you can use a pivot table if you're in Excel. It looks complicated, but it's just matching up the `human_label` and `llm_category` column and seeing how often they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.human_label, df.llm_category, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, environmental bills often got confused for bills about taxes/fines. Healthcare came out with a 100% score\n",
    "\n",
    "> I said \"in this case,\" but the answers actually change if you re-run the LLM category assignment! Even with a small 88-row dataset the LLM will change its mind on some of them.\n",
    "\n",
    "If you were going to do this in Google Sheets, it would be the same thing! You'd just add a `human_label` column, then do a pivot table to see if they match. I [wrote a little Apps Script](https://gist.github.com/jsoma/06783a9e759003e2e69389d677f83c0f) that adds a helper for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured outputs\n",
    "\n",
    "Honestly, it did a remarkably good job at returning the results we're looking for! In other situations it's been a little more unpredictable for me. One way to force the AI to return what you're looking for is using a tool like [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs) to demand OpenAI gives you the exact response you're looking for.\n",
    "\n",
    "> If you aren't technical: using ChatGPT is fine and good because as a human it's easy to understand the response. The difference between getting \"YES, a donut\" as a response and \"Yes - a donut\" is meaningles to a person, but they're very different things when you're a computer! Trying to automatically parse responses when the LLM can decide to go rogue can be a real pain in the neck.\n",
    " \n",
    "If you're using a non-OpenAI LLM, try out [Instructor](https://python.useinstructor.com/). If you're using a local model, [Outlines](https://dottxt-ai.github.io/outlines/latest/) is for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by setting up the structure we'd like our response to be in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class Comment(BaseModel):\n",
    "    \"\"\"Data model for a comment.\"\"\"\n",
    "    name: str = Field(description=\"Author's name\")\n",
    "    food_item: str = Field(description=\"Food item being discussed, in English\")\n",
    "    email: str = Field(description=\"Author's email\")\n",
    "    emotion: Literal[\"positive\", \"negative\", \"neutral\"] = Field(description=\"Comment sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now we'll run it against a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'XXXXXXXX'\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = \"\"\"\n",
    "After the broccoli incident, I never want to look at broccoli again. Please remove me \n",
    "from the broccoli email list.\n",
    "\n",
    "Sincerely,\n",
    "Jackary Baloneynose\n",
    "jackary.baloneynose@example.com\n",
    "\"\"\"\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the relevant information.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    response_format=Comment,\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.parsed\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want it as a Python object, we just pop `.model_dump()` on the end of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Let's say we have a list of comments that we'd like to extract some data from. Maybe put into a few categories, pull out an email and a name, maybe get the language, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 300\n",
    "\n",
    "df = pd.read_csv(\"data/comments.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take everything we've done so far and put it together:\n",
    "\n",
    "1. Write a pydantic model\n",
    "2. Build a function to re-use our LLM query code\n",
    "3. Loop through the dataframe to get the details for each row\n",
    "\n",
    "At the end, we'll have a nice new dataframe of just the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "class Comment(BaseModel):\n",
    "    \"\"\"Data model for a comment.\"\"\"\n",
    "    name: str = Field(description=\"Author's name\")\n",
    "    food_item: str = Field(description=\"Food item being discussed, in English\")\n",
    "    email: str = Field(description=\"Author's email\")\n",
    "    emotion: Literal[\"positive\", \"negative\", \"neutral\"] = Field(description=\"Comment sentiment\")\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'XXXXXXXX'\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def llm_query(row):\n",
    "    try:\n",
    "        prompt = f\"\"\"\n",
    "        EMAIL CONTENT:\n",
    "        {row['contents']}\n",
    "        \"\"\"\n",
    "        \n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Extract the relevant information.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_format=Comment,\n",
    "        )\n",
    "        \n",
    "        response = completion.choices[0].message.parsed\n",
    "        return pd.Series(response.model_dump())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return pd.Series({})\n",
    "\n",
    "responses = df.progress_apply(llm_query, axis=1)\n",
    "responses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can join it with our original dataframe and have a nice expanded set of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df.join(responses.add_prefix('llm_'))\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting and adapting\n",
    "\n",
    "If we want to use this on our own, we need to change a few things: we need a **new dataset**, a **new pydantic model** and **to adjust our prompt**.\n",
    "\n",
    "Let's say we've migrated to analyzing documents in a handful of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/articles.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new model\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List\n",
    "\n",
    "class ArticleSource(BaseModel):\n",
    "    \"\"\"Data model for a single source in an article.\"\"\"\n",
    "    name: str = Field(description=\"Source's name, in English\")\n",
    "    position_or_title: str = Field(description=\"Source's position or title\")\n",
    "    \n",
    "class ArticleDetails(BaseModel):\n",
    "    \"\"\"Data model for an article.\"\"\"\n",
    "    headline: str = Field(description=\"Article headline\")\n",
    "    outlet_name: str = Field(description=\"News outlet name\")\n",
    "    summary: str = Field(description=\"Three-sentence summary, in English\")\n",
    "    language: str = Field(description=\"Two-letter language code of original article\")\n",
    "    sources: List[ArticleSource] = Field(description=\"People quoted in the article\")\n",
    "    category: Literal[\"politics\", \"sports\", \"entertainment\", \"other\"] = Field(description=\"Article category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new prompt - pay attention to your column names and response_format!!!\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'XXXXXXXX'\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def llm_query(row):\n",
    "    try:\n",
    "        prompt = f\"\"\"\n",
    "        ARTICLE URL: {row['URL']}\n",
    "        ARTICLE CONTENT:\n",
    "        {row['article_text']}\n",
    "        \"\"\"\n",
    "        \n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Extract the relevant information.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_format=ArticleDetails,\n",
    "        )\n",
    "        \n",
    "        response = completion.choices[0].message.parsed\n",
    "        return pd.Series(response.model_dump())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return pd.Series({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's run it\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "responses = df.progress_apply(llm_query, axis=1)\n",
    "responses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and save\n",
    "merged = df.join(responses.add_prefix('llm_'))\n",
    "merged.to_csv(\"articles-with-details.csv\", index=False)\n",
    "\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "In this notebook we looked at how **AI isn't just one *thing*, it's has all sorts of versions and options same as everything else on the planet.** Even ChatGPT has different versions - 4o-mini, 4o, 4.5... of which 4o is more powerful in a handful of ways, but 4o-mini is good enough for most of the work we'll be doing.\n",
    "\n",
    "A simple task for AI is putting things in categories, also known as classification. When doing bulk classification, it's important to **examine the outputs systemtically** and not just spot check! That way you know how or why the AI might be going wrong, and either tweak your prompt or build knowledge of the mistakes into your process.\n",
    "\n",
    "**Finally, LLMs don't always listen to your rules.** They might respond in formats you didn't ask for, add extra categories, or overrule the rules you specified. Using a temperature of 0 and structured outputs tools Structured Outputs and Instructor help keep responses in line with your rules to make your analysis process that much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
